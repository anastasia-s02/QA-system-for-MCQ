{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Transforming our data**\n",
        "\n",
        "Note: This code is specified for TOEFL and RACE dataset in our specific format (json) in which each instance is separated by a comma (\",\"). Further, each element in the instance is sepated by \"<delimiter!>\", except for the last one (the explantion for the correct answer) which is separated by \":\".\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "If your Data file is in the same format you can use following code without alterations:"
      ],
      "metadata": {
        "id": "_B2D2pskqADR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import re"
      ],
      "metadata": {
        "id": "KmDwObfnp7_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This approach separates each instance into 6 parts - id, context, question, answer choices, correct answer, and explantion. The output file contains same information (with numerical id) but in this data file **all** elements (including the explanation for the correct answer) are separated by \"<delimiter!>\"."
      ],
      "metadata": {
        "id": "SRS40Kyys53q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7brtFhXlhxg",
        "outputId": "fa3552bb-3bf8-4cfd-fc64-5b80032aa9a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "690\n"
          ]
        }
      ],
      "source": [
        "def process_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    processed_data = []\n",
        "    for idx, (key, value) in enumerate(data.items(), start=1):\n",
        "        parts = key.split('<delimiter!>')\n",
        "        #print(parts)\n",
        "        context = parts[0].strip()\n",
        "        question = parts[1].strip()\n",
        "\n",
        "        # Splitting answer choices using regular expressions\n",
        "        answer_choices_str = parts[2].strip()\n",
        "        answer_choices = re.split(r'\\s(?=[A-D]\\.)', answer_choices_str)\n",
        "        answer_choices = [choice.strip() for choice in answer_choices]\n",
        "\n",
        "        correct_answer = parts[3].strip()\n",
        "\n",
        "        processed_data.append({\n",
        "            'id': idx,\n",
        "            'context': context,\n",
        "            'question': question,\n",
        "            'answer_choices': answer_choices,\n",
        "            'correct_answer': correct_answer,\n",
        "            'explanation': value\n",
        "        })\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "# Let's load our dataset\n",
        "file_path = '/content/train_gpt_TOEFL_verified.json'  #input the dataset file path here\n",
        "processed_data = process_file(file_path)\n",
        "print(len(processed_data)) # to see how many Q&A we have to go through"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output=[]\n",
        "\n",
        "for item in processed_data:\n",
        "  id = item['id']\n",
        "  context = item['context'].replace('\\n', ' ')\n",
        "  question = item['question']\n",
        "  answer_choices = item['answer_choices']\n",
        "  correct_answer = item['correct_answer']\n",
        "  explanation = item['explanation']\n",
        "\n",
        "  answer_choices_string = ' '.join(item['answer_choices'])\n",
        "\n",
        "  formatted_string = f\"{id}<delimiter!>{context}<delimiter!>{question}<delimiter!>{answer_choices_string}<delimiter!>{correct_answer}<delimiter!>{explanation}\"\n",
        "  output.append(formatted_string)\n",
        "\n",
        "  #print(id) # to see what step we are on"
      ],
      "metadata": {
        "id": "q6AIdpxwzBBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('train_gpt_TOEFL_verified_processed_all.json', 'w') as file:\n",
        "    json.dump(output, file)"
      ],
      "metadata": {
        "id": "-SkdM8eoz4OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This approach separates each instance into 3 parts as required by contrastive learning - key (which includes context and question), positive answers (which includes the correct answer and the explanation for it), and negative answers (which includes the incorrect answer choices). The resulting file cointains elements separated by \",\" as in the input file but in this case all the 3 parts of each element are separated by \"<delimiter!>\"."
      ],
      "metadata": {
        "id": "tNFtsdags4Se"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "letter_to_num = {\"A\":0, \"B\":1, \"C\":2, \"D\":3} # to convert letter to answer\n",
        "\n",
        "def process_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    processed_data = []\n",
        "    for idx, (key, value) in enumerate(data.items(), start=1):\n",
        "        parts = key.split('<delimiter!>')\n",
        "        context = parts[0].strip()\n",
        "        question = parts[1].strip()\n",
        "\n",
        "        # Splitting answer choices using regular expressions\n",
        "        answer_choices_str = parts[2].strip()\n",
        "        answer_choices = re.split(r'\\s(?=[A-D]\\.)', answer_choices_str)\n",
        "        answer_choices = [choice.strip() for choice in answer_choices]\n",
        "\n",
        "        correct_answer = parts[3].strip()\n",
        "\n",
        "        positive_answer = answer_choices[letter_to_num[correct_answer]]\n",
        "\n",
        "        negative_answer = [x for x in answer_choices if x != positive_answer]\n",
        "\n",
        "        processed_data.append({\n",
        "            'key': context + \":\" + question,\n",
        "            'positive_answers': positive_answer+\":\"+value,\n",
        "            'negative_answers': negative_answer\n",
        "        })\n",
        "\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "# Let's load our first dataset (RACE test dataset) to test the MQAG model on\n",
        "file_path = '/content/train_gpt_TOEFL_verified.json'\n",
        "processed_data = process_file(file_path)\n",
        "print(len(processed_data)) # to see how many Q&A we have to go through"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elG5gDTwlki5",
        "outputId": "ce365dd8-547a-4e14-eb4a-d945cdf89659"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "690\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output=[]\n",
        "\n",
        "for item in processed_data:\n",
        "  key = item['key']\n",
        "  positive_answers = item['positive_answers']\n",
        "  negative_answers = ' '.join(item['negative_answers'])\n",
        "\n",
        "  formatted_string = f\"{key}<delimiter!>{positive_answers}<delimiter!>{negative_answers}\"\n",
        "  output.append(formatted_string)\n",
        "\n",
        "  #print(id) # to see what step we are on"
      ],
      "metadata": {
        "id": "h-Hsefe4lkc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('train_gpt_TOEFL_verified_processed_key_pos_neg.json', 'w') as file:\n",
        "    json.dump(output, file)"
      ],
      "metadata": {
        "id": "AWvaUr-rlkZ5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}